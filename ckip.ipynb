{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b851a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('comments_data/all_comments.csv', encoding='utf-8')\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390db72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_list(data):\n",
    "    data['author_name'] = data['author_name'].apply(lambda x: str(x).replace('@', ''))\n",
    "\n",
    "    name_list = []\n",
    "    for i, name in enumerate(data['author_name']):\n",
    "        name_str = str(name)\n",
    "        name_list.append(name_str)\n",
    "    \n",
    "    print(\">>> name_list:\", name_list)\n",
    "\n",
    "    return name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取作者名稱\n",
    "name_data = name_list(df)\n",
    "print(f\"總共有 {len(name_data)} 個不重複的留言者\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974bb7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(comment_data, name_list):\n",
    "    print(\"-\"*20)\n",
    "    print(\">>> 清理留言文字\")\n",
    "    comment_data['cleaned_text'] = None\n",
    "\n",
    "    for i, c in enumerate(comment_data['comment_text']):\n",
    "        # print(f\"{i+1} / {len(comment_data)}\")\n",
    "\n",
    "        # 移除 HTML 標籤、特殊符號、author_name\n",
    "        c1 = re.sub(r'<br>|<a href=\".*?\">.*?</a>|<\\/?b>', ' ', str(c))\n",
    "        c2 = re.sub(r'[^\\w\\s,]', ' ', c1)\n",
    "        c3 = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fa5]', ' ', c2)\n",
    "        \n",
    "        for name in name_list:\n",
    "            c_cleaned = re.sub(r'\\b' + re.escape(name) + r'\\b', '', c3)\n",
    "\n",
    "        comment_data.at[i, 'cleaned_text'] = c_cleaned\n",
    "        # print(f\"{c} -> {c_cleaned}\")\n",
    "    \n",
    "    cleaned_data = comment_data.to_csv('comments_data/cleaned_comments.csv', index=False)\n",
    "\n",
    "    return drop_empty_comments_summary(cleaned_data)\n",
    "\n",
    "def drop_empty_comments_summary(df):\n",
    "    print(\"-\"*20)\n",
    "    print(\">>> 資料清洗後去除空白留言\")\n",
    "    original = df.groupby('video_title').size()\n",
    "    print(\">>> 原始留言數量：\", original)\n",
    "\n",
    "    # 去除空留言\n",
    "    df_cleaned = df.dropna(subset=['cleaned_text'])\n",
    "    df_cleaned = df_cleaned[df_cleaned['cleaned_text'].str.strip() != '']\n",
    "\n",
    "    # 留下來的留言數\n",
    "    after_drop = df_cleaned.groupby('video_title').size()\n",
    "    print(\">>> 去除空白留言數量：\", after_drop)\n",
    "\n",
    "    # 對齊 index，沒有的補 0\n",
    "    after_drop = after_drop.reindex(original.index, fill_value=0)\n",
    "\n",
    "    # 相減：被刪除的筆數\n",
    "    drop_count = (original - after_drop).reset_index()\n",
    "    drop_count.columns = ['video_title', 'count']\n",
    "\n",
    "    # save diff and dropped data to csv\n",
    "    df_cleaned.to_csv('comments_data/dropped_ckip_comments.csv', index=False)\n",
    "    drop_count.to_csv('comments_data/dropped_summary.csv', index=False)\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a22c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理留言文字、移除空白row\n",
    "\n",
    "df_cleaned = clean_text(df, name_list)\n",
    "df_cleaned[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb60cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ckip(cleaned_data):\n",
    "    # load ckiptagger files from google drive\n",
    "    # data_utils.download_data_gdown(\"./\") \n",
    "\n",
    "    # load ckiptagger model\n",
    "    ws = WS(\"./data\")\n",
    "    pos = POS(\"./data\")\n",
    "    ner = NER(\"./data\")\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"開始分析 ckip\")\n",
    "    \n",
    "    ckip_data = cleaned_data.reset_index(drop=True)\n",
    "\n",
    "    ckip_data['ws'] = None\n",
    "    ckip_data['pos'] = None\n",
    "    ckip_data['ner'] = None\n",
    "\n",
    "\n",
    "    grouped = ckip_data.groupby('video_title')\n",
    "    for idx, (video_title, group) in enumerate(grouped):\n",
    "        print(f\"=== 處理影片: {video_title} (index: {idx}) ===\")\n",
    "        group = group.reset_index(drop=True)\n",
    "        for i, text in enumerate(group['cleaned_text']):\n",
    "            print(f\">>> Progressing: {i + 1} / {len(group)}\")\n",
    "            try:\n",
    "                if not isinstance(text, str) or text.strip() == '':\n",
    "                    continue\n",
    "\n",
    "                # CKIP: WS\n",
    "                try:\n",
    "                    ws_result = ws([text])\n",
    "                    ws_tokens = ws_result[0]\n",
    "                    group.at[i, 'ws'] = ws_tokens\n",
    "                except Exception as e:\n",
    "                    print(f\"[WS Error] 第{i}行: {e}\")\n",
    "                    continue  # 若 WS 錯，無法進行 POS/NER，直接跳過\n",
    "\n",
    "                # CKIP: POS\n",
    "                try:\n",
    "                    pos_result = pos([ws_tokens])\n",
    "                    group.at[i, 'pos'] = pos_result[0]\n",
    "                except Exception as e:\n",
    "                    print(f\"[POS Error] 第{i}行: {e}\")\n",
    "\n",
    "                # CKIP: NER\n",
    "                try:\n",
    "                    ner_result = ner([ws_tokens], [pos_result[0]])\n",
    "                    group.at[i, 'ner'] = list(ner_result[0])\n",
    "                except Exception as e:\n",
    "                    print(f\"[NER Error] 第{i}行: {e}\")\n",
    "\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[General Error] 第{i}行發生錯誤: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 儲存每集結果\n",
    "        group.to_csv(f'comments_data/for_bert/video_{idx}_ckip.csv', index=False)\n",
    "        print(f\"=== CKIP Done, save to video_{idx}_ckip.csv\")\n",
    "\n",
    "    # 合併所有 group 結果\n",
    "    ckip_data = pd.concat([group for _, group in grouped], ignore_index=True)\n",
    "\n",
    "    # 儲存結果\n",
    "    ckip_data.to_csv('comments_data/for_bert/ckip_comments.csv', index=False)\n",
    "    # ckip_data.to_csv('../comments_data/comments/df1_ckip.csv', index=False)\n",
    "    print(\"=== CKIP Done, save to ckip_comments.csv\")\n",
    "\n",
    "    return ckip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee030a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckip_data = process_ckip(df_cleaned)\n",
    "print(ckip_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03602140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df):\n",
    "    # load stopword.txt\n",
    "    with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "        stopwords = [word.strip() for word in stopwords]\n",
    "        print(stopwords)\n",
    "    # remove stopwords\n",
    "    for i, tokens in enumerate(df['ws']):\n",
    "        if isinstance(tokens, list):\n",
    "            df.at[i, 'ws_cleaned'] = [t for t in tokens if t.strip() not in stopwords]\n",
    "            ws_str = ' '.join(df.at[i, 'ws_cleaned'])\n",
    "            df.at[i, 'ws_str'] = ws_str\n",
    "        else:\n",
    "            df.at[i, 'ws_cleaned'] = tokens\n",
    "            df.at[i, 'ws_str'] = tokens\n",
    "\n",
    "    # save to csv\n",
    "    df.to_csv('comments_data/ws_str_comments.csv', index=False)\n",
    "    print(\"=== Removed stopwords, save to ws_str_comments.csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd54726",
   "metadata": {},
   "outputs": [],
   "source": [
    "claned_str = remove_stopwords(ckip_data)\n",
    "print(claned_str.head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
